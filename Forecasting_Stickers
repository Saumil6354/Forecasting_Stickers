{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10574734,"sourceType":"datasetVersion","datasetId":6543809},{"sourceId":10574783,"sourceType":"datasetVersion","datasetId":6543842},{"sourceId":219142116,"sourceType":"kernelVersion"},{"sourceId":241482,"sourceType":"modelInstanceVersion","modelInstanceId":206299,"modelId":228051}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split, cross_val_predict\nfrom sklearn.metrics import make_scorer, mean_absolute_percentage_error\nfrom sklearn.linear_model import LinearRegression\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import StackingRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import cross_val_score, KFold\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import Ridge \n\ntrain = pd.read_csv('/kaggle/input/train2/train.csv')\ntest = pd.read_csv('/kaggle/input/train2/test.csv')\nsubmission = pd.read_csv('/kaggle/input/train2/sample_submission.csv')\n\ntrain.head()\n\ntrain.info()\n\ntrain = train.dropna()\n\ndef transform_date(df, col):\n    df[col] = pd.to_datetime(df[col])\n    \n    df['year'] = df[col].dt.year.astype('int')\n    df['quarter'] = df[col].dt.quarter.astype('int')\n    df['month'] = df[col].dt.month.astype('int')\n    df['day'] = df[col].dt.day.astype('int')\n    df['day_of_week'] = df[col].dt.dayofweek.astype('int')\n    df['week_of_year'] = df[col].dt.isocalendar().week.astype('int')\n    \n    df['day_sin'] = np.sin(2 * np.pi * df['day'] / 365)\n    df['day_cos'] = np.cos(2 * np.pi * df['day'] / 365)\n    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n    df['year_sin'] = np.sin(2 * np.pi * df['year'] / 7)\n    df['year_cos'] = np.cos(2 * np.pi * df['year'] / 7)\n    \n    df['group'] = (df['year'] - 2010) * 48 + df['month'] * 4 + df['day'] // 7\n    \n    return df\n\ntrain = transform_date(train, 'date')\ntest = transform_date(test, 'date')\n\ntrain = train.drop(columns=['date'], axis=1)\ntest = test.drop(columns=['date'], axis=1)\n\ncat_cols = ['country','store','product']\n\nlabel_encoders = {}  \nfor col in cat_cols:\n    le = LabelEncoder()\n    train[col] = le.fit_transform(train[col])\n    label_encoders[col] = le\n    \nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize=(8, 6))\nsns.histplot(train['num_sold'], kde=True, bins=30, color='violet')\n\nplt.title('Distribution of Sticker Sales (num_sold)', fontsize=16)\nplt.xlabel('Number of Stickers Sold')\nplt.ylabel('Frequency')\n\nplt.show()\n\n\ntrain['num_sold'] = np.log1p(train['num_sold'])\n\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize=(8, 6))\nsns.histplot(train['num_sold'], kde=True, bins=30, color='violet')\n\nplt.title('Distribution of Sticker Sales (num_sold)', fontsize=16)\nplt.xlabel('Number of Stickers Sold')\nplt.ylabel('Frequency')\n\nplt.show()\n\nX = train.drop(columns=['num_sold'])\ny = train['num_sold']\nX_train, X_valid, y_train, y_valid  = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndef tune_xgb(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 500, 4000),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-6, 1e-2),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-6, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-6, 10.0),\n        'random_state': 42,\n        'tree_method': 'gpu_hist',\n        'predictor': 'gpu_predictor',\n        'eval_metric': 'mape'\n    }\n    model = XGBRegressor(**params)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_percentage_error(y_valid, preds)\n    \n#%%time\n#study_xgb = optuna.create_study(direction='minimize')\n#study_xgb.optimize(tune_xgb, n_trials=50)\n\n#print(\"Best XGBoost params:\", study_xgb.best_params)\n#print(\"XGBoost MAPE:\", study_xgb.best_value)\n\ndef tune_lgb(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 500, 4000),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-6, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-6, 10.0),\n        'random_state': 42,\n        'device': 'gpu',\n        'metric': 'mape'\n    }\n    model = LGBMRegressor(**params)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_percentage_error(y_valid, preds)\n    \n#%%time\n#study_lgb = optuna.create_study(direction='minimize')\n#study_lgb.optimize(tune_lgb, n_trials=50)\n\n#print(\"Best LightGBM params:\", study_lgb.best_params)\n#print(\"LightGBM MAPE:\", study_lgb.best_value)\n\ndef tune_catboost(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 500, 4000),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n        'depth': trial.suggest_int('depth', 3, 15),\n        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-6, 10.0),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n        'random_strength': trial.suggest_float('random_strength', 0.0, 1.0),\n        'loss_function': 'MAPE',\n        'eval_metric': 'MAPE',\n        'random_state': 42\n    }\n    model = CatBoostRegressor(**params, verbose=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_percentage_error(y_valid, preds)\n    \n#%%time\n#study_catboost = optuna.create_study(direction='minimize')\n#study_catboost.optimize(tune_catboost, n_trials=50)\n\n#print(\"Best CatBoost params:\", study_catboost.best_params)\n#print(\"CatBoost MAPE:\", study_catboost.best_value)\n\nxgb_model = XGBRegressor(\n    n_estimators=3750,\n    learning_rate=0.022928762781800033,\n    max_depth=7,\n    min_child_weight=6,\n    subsample=0.9684855394596099,\n    colsample_bytree=0.7414559465626035,\n    gamma=0.0002633815873971183,\n    reg_alpha=0.033856532345376285,\n    reg_lambda=3.848636177756615e-05,\n    random_state=42\n)\nlgb_model = LGBMRegressor(\n    n_estimators=3130,\n    learning_rate=0.06759246686506182,\n    max_depth=13,\n    min_child_samples=13,\n    colsample_bytree=0.6725055033032713,\n    subsample=0.8760367415449087,\n    reg_alpha=0.13242379471194435,\n    reg_lambda=8.10849734527323e-06,\n    random_state=42\n)\n\ncatboost_model = CatBoostRegressor(\n    n_estimators=3671,\n    learning_rate=0.042483634105534726,\n    depth=8,\n    l2_leaf_reg=0.00045721354980041677,\n    bagging_temperature=0.6630177442611984,\n    random_strength=0.4765511332665974,\n    verbose=0,  # To suppress training logs\n    random_seed=42\n)\n\n#%%time\nmeta_model = LinearRegression()\nstacking_model = StackingRegressor(\n    estimators=[('xgb', xgb_model), ('lgb', lgb_model), ('catboost', catboost_model)],\n    final_estimator=meta_model,\n    n_jobs=-1\n)\n\nstacking_model.fit(X,y)\n\nlabel_encoders = {}  \nfor col in cat_cols:\n    le = LabelEncoder()\n    test[col] = le.fit_transform(test[col])\n    label_encoders[col] = le\ntest.head()\n\nsubmission_ids = test['id']\npredictions = stacking_model.predict(test)\npredictions = np.expm1(predictions)\nsubmission = pd.DataFrame({\n    'id': submission_ids,\n    'num_sold': predictions \n})\nsubmission.to_csv('sample_submission.csv', index=False)\nprint(\"File Saved!\")\nprint(submission.head())\n","metadata":{"execution":{"iopub.status.busy":"2025-01-25T04:28:36.470430Z","iopub.execute_input":"2025-01-25T04:28:36.470894Z","iopub.status.idle":"2025-01-25T04:47:34.851481Z","shell.execute_reply.started":"2025-01-25T04:28:36.470855Z","shell.execute_reply":"2025-01-25T04:47:34.849229Z"},"trusted":true},"outputs":[],"execution_count":null}]}